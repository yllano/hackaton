{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a12d5300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando modelo XGBoost y variables de prueba...\n",
      "✅ Modelo XGBoost re-entrenado y X_test/Y_test cargados.\n",
      "   RMSE (Error de Predicción): 13.11\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS NECESARIOS ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Suponiendo que df_master ya está cargado. Si no, descomenta la siguiente línea:\n",
    "df_master = pd.read_csv('../01_data/output/master_df.csv') \n",
    "# (Asegúrate de que la ruta sea correcta desde tu ubicación actual)\n",
    "\n",
    "print(\"Inicializando modelo XGBoost y variables de prueba...\")\n",
    "\n",
    "# 1. Preparación de Variables (One-Hot Encoding para XGBoost)\n",
    "# Se replican los pasos de la Fase 3\n",
    "df_model_full = pd.get_dummies(df_master.drop(columns=['InvoiceDate', 'Local_Currency']), \n",
    "                               columns=['StockCode', 'Country'])\n",
    "\n",
    "# 2. Definición de Features (X) y Target (Y)\n",
    "X = df_model_full.drop(columns=['Total_Quantity_Sold'])\n",
    "Y = df_model_full['Total_Quantity_Sold']\n",
    "\n",
    "# 3. División Entrenamiento/Prueba\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42 # Usamos el mismo random_state para consistencia\n",
    ")\n",
    "\n",
    "# 4. Re-entrenar el Modelo (Define xgb_model)\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_model.fit(X_train, Y_train)\n",
    "\n",
    "# Verificación de Precisión (Opcional, pero bueno tenerla)\n",
    "Y_pred = xgb_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "print(\"✅ Modelo XGBoost re-entrenado y X_test/Y_test cargados.\")\n",
    "print(f\"   RMSE (Error de Predicción): {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86dc4f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PERFIL DE LOS CLUSTERS DE CLIENTES (RFM) ---\n",
      "            Recency  Frequency    Monetary\n",
      "Cluster                                   \n",
      "0         14.434708   9.618557  701.835052\n",
      "1        171.679466   1.158598   27.923205\n",
      "2         16.837877   1.807747   37.793400\n",
      "3         75.125133   3.018028  160.952280\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd \n",
    "import numpy as np # <-- Necesario para np.log\n",
    "\n",
    "# --- PREPROCESAMIENTO ---\n",
    "df_master['InvoiceDate'] = pd.to_datetime(df_master['InvoiceDate'])\n",
    "\n",
    "# 1. Cálculo de RFM\n",
    "SNAPSHOT_DATE = df_master['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "df_rfm = df_master.groupby('CustomerID').agg(\n",
    "    Recency=('InvoiceDate', lambda x: (SNAPSHOT_DATE - x.max()).days),\n",
    "    Frequency=('InvoiceDate', 'nunique'),\n",
    "    Monetary=('Total_Quantity_Sold', 'sum') \n",
    ").reset_index()\n",
    "\n",
    "# --- 1.5 TRANSFORMACIÓN LOGARÍTMICA (PASO CLAVE PARA K-MEANS) ---\n",
    "# Se aplica a todas las métricas, especialmente Frequency y Monetary, para reducir el sesgo.\n",
    "df_rfm['Recency_Log'] = np.log(df_rfm['Recency'] + 1)\n",
    "df_rfm['Frequency_Log'] = np.log(df_rfm['Frequency'] + 1)\n",
    "df_rfm['Monetary_Log'] = np.log(df_rfm['Monetary'] + 1)\n",
    "\n",
    "# 2. Segmentación con Cuartiles (Puntuación simple)\n",
    "# Se usa la métrica original para las puntuaciones, ya que queremos puntuar los valores reales.\n",
    "\n",
    "# R_Score: Se mantiene en 5 cuartiles\n",
    "df_rfm['R_Score'] = pd.qcut(\n",
    "    df_rfm['Recency'], \n",
    "    5, \n",
    "    labels=[5, 4, 3, 2, 1],\n",
    "    duplicates='drop'\n",
    ") \n",
    "\n",
    "# F_Score: Usa pd.cut con 3 grupos fijos por el sesgo extremo (solución al ValueError)\n",
    "min_freq = df_rfm['Frequency'].min()\n",
    "max_freq = df_rfm['Frequency'].max()\n",
    "bins_f = [min_freq - 1, 1, 3, max_freq] \n",
    "labels_f = [1, 2, 3] \n",
    "\n",
    "df_rfm['F_Score'] = pd.cut(\n",
    "    df_rfm['Frequency'], \n",
    "    bins=bins_f, \n",
    "    labels=labels_f,\n",
    "    include_lowest=True,\n",
    "    right=True\n",
    ")\n",
    "\n",
    "# M_Score: 3 cuartiles (terciles) para consistencia\n",
    "df_rfm['M_Score'] = pd.qcut(\n",
    "    df_rfm['Monetary'], \n",
    "    3, \n",
    "    labels=[1, 2, 3], \n",
    "    duplicates='drop'\n",
    ")\n",
    "\n",
    "df_rfm['RFM_Score'] = df_rfm['R_Score'].astype(str) + df_rfm['F_Score'].astype(str) + df_rfm['M_Score'].astype(str)\n",
    "\n",
    "# 3. Clustering K-Means (Opción 2: ML Avanzado)\n",
    "# Escalar datos para K-Means\n",
    "# ¡IMPORTANTE! Usamos las columnas _Log para el clustering.\n",
    "X_rfm = df_rfm[['Recency_Log', 'Frequency_Log', 'Monetary_Log']] \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_rfm)\n",
    "\n",
    "# Aplicar K-Means\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "df_rfm['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 4. Interpretación del Clustering\n",
    "# Para interpretar los clústeres, usamos las métricas ORIGINALES (sin el log)\n",
    "# ya que representan el valor real del negocio.\n",
    "cluster_profile = df_rfm.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean()\n",
    "print(\"\\n--- PERFIL DE LOS CLUSTERS DE CLIENTES (RFM) ---\")\n",
    "print(cluster_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7e24ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency\n",
      "1     1463\n",
      "2      616\n",
      "3      443\n",
      "4      260\n",
      "5      170\n",
      "6      119\n",
      "7       67\n",
      "8       59\n",
      "10      37\n",
      "9       36\n",
      "Name: count, dtype: int64\n",
      "0.25      1.0\n",
      "0.50      2.0\n",
      "0.75      4.0\n",
      "1.00    140.0\n",
      "Name: Frequency, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Muestra los 10 valores de frecuencia más comunes\n",
    "print(df_rfm['Frequency'].value_counts().head(10))\n",
    "# Muestra los percentiles para ver los cortes\n",
    "print(df_rfm['Frequency'].quantile([0.25, 0.5, 0.75, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8046071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd \n",
    "\n",
    "# --- PREPROCESAMIENTO ---\n",
    "df_master['InvoiceDate'] = pd.to_datetime(df_master['InvoiceDate'])\n",
    "\n",
    "# 1. Cálculo de RFM\n",
    "SNAPSHOT_DATE = df_master['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "df_rfm = df_master.groupby('CustomerID').agg(\n",
    "    Recency=('InvoiceDate', lambda x: (SNAPSHOT_DATE - x.max()).days),\n",
    "    Frequency=('InvoiceDate', 'nunique'),\n",
    "    Monetary=('Total_Quantity_Sold', 'sum') \n",
    ").reset_index()\n",
    "\n",
    "# 2. Segmentación con Cuartiles (Opción 1: Corrección Final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "924201ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el Costo (Reutilizamos el costo fijo que ya calculaste)\n",
    "average_price = df_master['UnitPrice'].mean()\n",
    "FIXED_COST = average_price * 0.50 \n",
    "\n",
    "def find_optimal_price_for_row(row, xgb_model, price_steps=50):\n",
    "    \"\"\"\n",
    "    Busca el precio óptimo para una única fila de datos (Producto/País/Día).\n",
    "    Devuelve el precio óptimo y la ganancia máxima esperada.\n",
    "    \"\"\"\n",
    "    best_profit = -np.inf\n",
    "    optimal_price = row['UnitPrice'] # Inicializamos con el precio actual\n",
    "\n",
    "    # Clonamos la fila y la convertimos en un DataFrame con una sola fila\n",
    "    # Esto es necesario porque el modelo XGBoost espera un DataFrame\n",
    "    features_to_predict = pd.DataFrame([row], columns=row.index)\n",
    "    \n",
    "    # Definir el rango de precios a probar (Ej. +/- 20% del precio actual)\n",
    "    current_price = row['UnitPrice']\n",
    "    price_range = np.linspace(\n",
    "        current_price * 0.8, \n",
    "        current_price * 1.2, \n",
    "        num=price_steps\n",
    "    )\n",
    "    \n",
    "    # 2. Bucle de Optimización (Prueba y Error)\n",
    "    for price in price_range:\n",
    "        # Actualizar el precio propio en las features clonadas\n",
    "        features_to_predict.loc[:, 'UnitPrice'] = price\n",
    "        \n",
    "        # Predecir la Demanda con el nuevo precio\n",
    "        # Usamos el modelo previamente entrenado (xgb_model)\n",
    "        predicted_demand = xgb_model.predict(features_to_predict)[0]\n",
    "        \n",
    "        # Calcular la Ganancia: (Precio - Costo) * Demanda\n",
    "        profit = (price - FIXED_COST) * predicted_demand\n",
    "        \n",
    "        # Actualizar el precio óptimo si se encuentra una mayor ganancia\n",
    "        if profit > best_profit:\n",
    "            best_profit = profit\n",
    "            optimal_price = price\n",
    "            \n",
    "    return optimal_price, best_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e92755f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando 500 recomendaciones de precios...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAMPLE_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m recomendaciones de precios...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 2. Aplicar la función a cada fila y generar las nuevas columnas\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Usamos un bucle for simple para aplicar la función a cada fila.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m results = \u001b[43mdf_recommendations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfind_optimal_price_for_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexpand\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m results.columns = [\u001b[33m'\u001b[39m\u001b[33mOptimal_Price\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMax_Profit\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 3. Combinar los resultados con las características originales\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yesic\\OneDrive\\Escritorio\\Hackaton\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:10401\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10387\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10389\u001b[39m op = frame_apply(\n\u001b[32m  10390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10391\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10399\u001b[39m     kwargs=kwargs,\n\u001b[32m  10400\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10401\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yesic\\OneDrive\\Escritorio\\Hackaton\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yesic\\OneDrive\\Escritorio\\Hackaton\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yesic\\OneDrive\\Escritorio\\Hackaton\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAMPLE_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m recomendaciones de precios...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 2. Aplicar la función a cada fila y generar las nuevas columnas\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Usamos un bucle for simple para aplicar la función a cada fila.\u001b[39;00m\n\u001b[32m     12\u001b[39m results = df_recommendations.apply(\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mfind_optimal_price_for_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[32m     14\u001b[39m     axis=\u001b[32m1\u001b[39m, result_type=\u001b[33m'\u001b[39m\u001b[33mexpand\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m results.columns = [\u001b[33m'\u001b[39m\u001b[33mOptimal_Price\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMax_Profit\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 3. Combinar los resultados con las características originales\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mfind_optimal_price_for_row\u001b[39m\u001b[34m(row, xgb_model, price_steps)\u001b[39m\n\u001b[32m     28\u001b[39m features_to_predict.loc[:, \u001b[33m'\u001b[39m\u001b[33mUnitPrice\u001b[39m\u001b[33m'\u001b[39m] = price\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Predecir la Demanda con el nuevo precio\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Usamos el modelo previamente entrenado (xgb_model)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m predicted_demand = \u001b[43mxgb_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_to_predict\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Calcular la Ganancia: (Precio - Costo) * Demanda\u001b[39;00m\n\u001b[32m     35\u001b[39m profit = (price - FIXED_COST) * predicted_demand\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yesic\\OneDrive\\Escritorio\\Hackaton\\venv\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yesic\\OneDrive\\Escritorio\\Hackaton\\venv\\Lib\\site-packages\\xgboost\\sklearn.py:1443\u001b[39m, in \u001b[36mXGBModel.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_use_inplace_predict():\n\u001b[32m   1442\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m         predts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmargin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1451\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[32m   1452\u001b[39m             cp = import_cupy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yesic\\OneDrive\\Escritorio\\Hackaton\\venv\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yesic\\OneDrive\\Escritorio\\Hackaton\\venv\\Lib\\site-packages\\xgboost\\core.py:2854\u001b[39m, in \u001b[36mBooster.inplace_predict\u001b[39m\u001b[34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[39m\n\u001b[32m   2852\u001b[39m     data, fns, _ = _transform_pandas_df(data, enable_categorical)\n\u001b[32m   2853\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[32m-> \u001b[39m\u001b[32m2854\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_list(data) \u001b[38;5;129;01mor\u001b[39;00m _is_tuple(data):\n\u001b[32m   2856\u001b[39m     data = np.array(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yesic\\OneDrive\\Escritorio\\Hackaton\\venv\\Lib\\site-packages\\xgboost\\core.py:3408\u001b[39m, in \u001b[36mBooster._validate_features\u001b[39m\u001b[34m(self, feature_names)\u001b[39m\n\u001b[32m   3402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3404\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdata did not contain feature names, but the following fields are expected: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3405\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.feature_names)\n\u001b[32m   3406\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_names\u001b[49m != feature_names:\n\u001b[32m   3409\u001b[39m     dat_missing = \u001b[38;5;28mset\u001b[39m(cast(FeatureNames, \u001b[38;5;28mself\u001b[39m.feature_names)) - \u001b[38;5;28mset\u001b[39m(\n\u001b[32m   3410\u001b[39m         cast(FeatureNames, feature_names)\n\u001b[32m   3411\u001b[39m     )\n\u001b[32m   3412\u001b[39m     my_missing = \u001b[38;5;28mset\u001b[39m(cast(FeatureNames, feature_names)) - \u001b[38;5;28mset\u001b[39m(\n\u001b[32m   3413\u001b[39m         cast(FeatureNames, \u001b[38;5;28mself\u001b[39m.feature_names)\n\u001b[32m   3414\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yesic\\OneDrive\\Escritorio\\Hackaton\\venv\\Lib\\site-packages\\xgboost\\core.py:2351\u001b[39m, in \u001b[36mBooster.feature_names\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2345\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeature_names\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Optional[FeatureNames]:\n\u001b[32m   2347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Feature names for this booster.  Can be directly set by input data or by\u001b[39;00m\n\u001b[32m   2348\u001b[39m \u001b[33;03m    assignment.\u001b[39;00m\n\u001b[32m   2349\u001b[39m \n\u001b[32m   2350\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_feature_info\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yesic\\OneDrive\\Escritorio\\Hackaton\\venv\\Lib\\site-packages\\xgboost\\core.py:2301\u001b[39m, in \u001b[36mBooster._get_feature_info\u001b[39m\u001b[34m(self, field)\u001b[39m\n\u001b[32m   2298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhandle\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2300\u001b[39m _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterGetStrFeatureInfo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2302\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43msarr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2306\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2307\u001b[39m )\n\u001b[32m   2308\u001b[39m feature_info = from_cstr_to_pystr(sarr, length)\n\u001b[32m   2309\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m feature_info \u001b[38;5;28;01mif\u001b[39;00m feature_info \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- APLICACIÓN MASIVA DEL MOTOR PRESCRIPTIVO ---\n",
    "\n",
    "# 1. Seleccionar un subconjunto de datos de prueba para la demo masiva\n",
    "# Usamos X_test (que contiene las features One-Hot Encoded)\n",
    "SAMPLE_SIZE = 500\n",
    "df_recommendations = X_test.head(SAMPLE_SIZE).copy()\n",
    "\n",
    "print(f\"Generando {SAMPLE_SIZE} recomendaciones de precios...\")\n",
    "\n",
    "# 2. Aplicar la función a cada fila y generar las nuevas columnas\n",
    "# Usamos un bucle for simple para aplicar la función a cada fila.\n",
    "results = df_recommendations.apply(\n",
    "    lambda row: find_optimal_price_for_row(row, xgb_model), \n",
    "    axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "results.columns = ['Optimal_Price', 'Max_Profit']\n",
    "\n",
    "# 3. Combinar los resultados con las características originales\n",
    "df_recommendations = pd.concat([df_recommendations, results], axis=1)\n",
    "\n",
    "# 4. Mapear las columnas clave para la visualización\n",
    "# Necesitamos mapear de vuelta a StockCode y Country para el reporte\n",
    "def get_original_id(row, prefix):\n",
    "    # Función para revertir el One-Hot Encoding y obtener el nombre original\n",
    "    cols = [col for col in row.index if col.startswith(prefix) and row[col] == 1]\n",
    "    return cols[0].split('_')[-1] if cols else 'N/A'\n",
    "\n",
    "df_recommendations['StockCode'] = df_recommendations.apply(lambda row: get_original_id(row, 'StockCode'), axis=1)\n",
    "df_recommendations['Country'] = df_recommendations.apply(lambda row: get_original_id(row, 'Country'), axis=1)\n",
    "\n",
    "\n",
    "# 5. Cálculo del Impacto Financiero\n",
    "df_recommendations['Current_Profit'] = (df_recommendations['UnitPrice'] - FIXED_COST) * Y_test.head(SAMPLE_SIZE)\n",
    "df_recommendations['Profit_Uplift_Pct'] = ((df_recommendations['Max_Profit'] - df_recommendations['Current_Profit']) / df_recommendations['Current_Profit']) * 100\n",
    "\n",
    "# 6. Presentación de la Tabla de Recomendaciones\n",
    "report_cols = ['StockCode', 'Country', 'UnitPrice', 'Optimal_Price', \n",
    "               'Current_Profit', 'Max_Profit', 'Profit_Uplift_Pct']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ TABLA DE PRECIOS RECOMENDADOS (MOTOR PRESCRIPTIVO A GRAN ESCALA)\")\n",
    "print(\"Las recomendaciones están basadas en maximizar la Ganancia por Transacción.\")\n",
    "print(\"=\"*80)\n",
    "print(df_recommendations[report_cols].sort_values(by='Profit_Uplift_Pct', ascending=False).head(10).to_markdown(index=False, floatfmt=\".2f\"))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Mejora Promedio de Ganancia en la muestra: {df_recommendations['Profit_Uplift_Pct'].mean():.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
